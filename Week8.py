# -*- coding: utf-8 -*-
"""Week_8_09_03_2023_NEW.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tbrYmh-hbG3TGr_3y9Noo7cdQe9fssWk

**Write a program to predict the eligibility of a customer for loan disbursement.**

**Process**

Steps are:
1.	Gathering Data
2.	Exploratory Data Analysis
3.	Data Visualizations
4.	Machine Learning Model Decision.
5.	Traing the ML Model
6.	Predict Model
7.	Deploy Model


**Important Libraries**


Python

Pandas

matplotlib 

seaborn

sklearn

# IMPORT PACKAGES
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

"""1.Gathering Data"""



"""Train file will be used for training the model, i.e. our model will learn from this file. It contains all the independent variables and the target variable.
Test file contains all the independent variables, but not the target variable. We will apply the model to predict the target variable for the test data.
"""

#  Create New Variable and stores the dataset values as Data Frame
# READING DATA
train=pd.read_csv("C:/MLDS CSV FILES/train.csv")
train.head()

test=pd.read_csv("C:/MLDS CSV FILES/test.csv")
test.head()

"""We can also use ***train*** to show few rows from the first five and last five record from the dataset"""

train # We have 614 rows and 13 columns in the train dataset.

test  #We have 367 rows and 12 columns in test dataset.

"""Here, we can see there are many rows and many columns, To know how many records and columns are available in our dataset, we can use the shape attribute or we can use len() to know how many records and how many features available in the dataset."""

print("Rows: ", len(train))

print("Columns: ", len(train.columns))

"""we can get the shape of the dataset using shape attribute

We can see there are three formats of data types:

**object**: Object format means variables are categorical. Categorical variables in our dataset are Loan_ID, Gender, Married, Dependents, Education, Self_Employed, Property_Area, Loan_Status.
**int64**: It represents the integer variables. ApplicantIncome is of this format.
**float64**: It represents the variable that has some decimal values involved. They are also numerical
"""

print(train.shape, test.shape)

"""After we collecting the data, Next step we need to understand what kind of data we have."""

train.columns  # print the list of columns

train_columns = train.columns # assign to a variable
train_columns # print the list of columns

"""Understanding the Data
First of all we use the ***train.describe()*** method to shows the important information from the dataset
It provides the count, mean, standard deviation (std), min, quartiles and max in its output.
"""

train.describe()

"""Another method is info(), This method show us the information about the dataset, Like
What's the type of culumn have?
How many rows available in the dataset?
What are the features are there?
How many null values available in the dataset?
Ans so on...
"""

train.info()

train['Loan_Status'].value_counts().plot.bar()

"""Categorical features: These features have categories (Gender, Married ,Self_Employed, Credit_History, Loan_Status)

Ordinal features: Variables in categorical features having some order involved
(Dependents, Education, Property_Area)


Numerical features: These features have numerical values (ApplicantIncome, CoapplicantIncome, LoanAmount, Loan_Amount_Term)

**Independent Variable (Categorical)**
"""

train['Gender'].value_counts(normalize=True).plot.bar(title='Gender') 
plt.show()
train['Married'].value_counts(normalize=True).plot.bar(title='Married')
plt.show()
train['Self_Employed'].value_counts(normalize=True).plot.bar(title='Self_employee')
plt.show()
train['Credit_History'].value_counts(normalize=True).plot.bar(title='Credit_History')
plt.show()

"""It can be inferred from the above bar plots that:

80% of applicants in the dataset are male.
Around 65% of the applicants in the dataset are married.
Around 15% of applicants in the dataset are self-employed.
Around 85% of applicants have repaid their doubts.

## **Independent Variable (Ordinal)**
"""

train['Dependents'].value_counts(normalize=True).plot.bar(title='Dependents')
plt.show()
train['Education'].value_counts(normalize=True).plot.bar(title='Education')
plt.show()
train['Property_Area'].value_counts(normalize=True).plot.bar(title='Property_Area')
plt.show()

"""The following inferences can be made from the above bar plots:

Most of the applicants don't have any dependents.
Around 80% of the applicants are Graduate.
Most of the applicants are from the Semiurban area.

## **Independent Variable (Numerical)**
Till now we have seen the categorical and ordinal variables and now let's visualize the numerical variables. Let's look at the distribution of Applicant income first.
"""

train['ApplicantIncome'].value_counts(normalize=True).plot.bar(title='ApplicantIncome')
plt.show()
train['CoapplicantIncome'].plot.box()
plt.show()
train['LoanAmount'].plot.box()
plt.show()

train.boxplot(column='ApplicantIncome', by = 'Education')
plt.suptitle(' ')
plt.show()

"""# Categorical Independent Variable vs Target Variable
First of all, we will find the relation between the target variable and categorical independent variables. Let us look at the stacked bar plot now which will give us the proportion of approved and unapproved loans.
"""

Gender=pd.crosstab(train['Gender'],train['Loan_Status'])
Gender.div(Gender.sum(1).astype(float),axis=0).plot(kind='bar',stacked=True,figsize=(4,4))

"""It can be inferred that the proportion of male and female applicants is more or less the same for both approved and unapproved loans.

## **Numerical Independent Variable vs Target Variable**
We will try to find the mean income of people for which the loan has been approved vs the mean income of people for which the loan has not been approved.
Here the y-axis represents the mean applicant income. We don’t see any change in the mean income. So, let’s make bins for the applicant income variable based on the values in it and analyze the corresponding loan status for each bin.
"""

train.groupby('Loan_Status')['ApplicantIncome'].mean().plot.bar()

bins=[0,2500,4000,6000,81000]
group=['Low','Average','High','Very High']
train['Income_bin']=pd.cut(train['ApplicantIncome'],bins,labels=group)
Income_bin=pd.crosstab(train['Income_bin'],train['Loan_Status'])
Income_bin.div(Income_bin.sum(1).astype(float),axis=0).plot(kind="bar",stacked=True)
plt.xlabel('ApplicantIncome')
P=plt.ylabel('Percentage')

matrix=train.corr()
f,ax=plt.subplots(figsize=(9,6))
sns.heatmap(matrix,vmax=.8,square=True,cmap="BuPu",annot=True)

train.isnull().sum()

train['Gender'].fillna(train['Gender'].mode()[0],inplace=True)
print(train['Gender'])

train['Married'].fillna(train['Married'].mode()[0],inplace=True)
print(train['Married'])

train['Dependents'].fillna(train['Dependents'].mode()[0],inplace=True)
print(train['Dependents'])

train['Self_Employed'].fillna(train['Self_Employed'].mode()[0],inplace=True)
print(train['Self_Employed'])

train['LoanAmount'].fillna(train['LoanAmount'].mode()[0],inplace=True)
print(train['LoanAmount'])

train['Loan_Amount_Term'].fillna(train['Loan_Amount_Term'].mean(),inplace=True)
print(train['LoanAmount'])

train['Credit_History'].fillna(train['Credit_History'].mean(),inplace=True)
print(train['Credit_History'])

# let's check whether all the missing values are filled in the dataset.
train.isnull().sum()

X=train.drop('Loan_Status',1)
#X=train.drop('Loan_ID')
X=X.drop('Loan_ID',axis=1)
Y=train['Loan_Status']

X=pd.get_dummies(X)

print(X)
print(Y)

"""## 2.** Exploratory Data Analysis**

In this section, We learn about extra information about data and it's characteristics.

First of all, We explore object type of data So let's make a function to know how many types of values available in the column


"""

def explore_object_type(df ,feature_name):
    """
    To know, How many values available in object('categorical') type of features
    And Return Categorical values with Count.
    """    
    if df[feature_name].dtype ==  'object':
        print(df[feature_name].value_counts())

# After defined a function, Let's call it. and check what's the output of our created function.
# Now, Test and Call a function for gender only
explore_object_type(train, 'Gender')

# Here's one little issue occurred, Suppose in your datasets there are lots of feature to defined like this above code.
# Solution is, Do you remember we have variable with name of `loan_train_columns`, Right,  let's use it
# 'Loan_ID', 'Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Property_Area', 'Loan_Status'

for featureName in train_columns:
    if train[featureName].dtype == 'object':
        print('\n"' + str(featureName) + '\'s" Values with count are :')
        explore_object_type(train, str(featureName))

"""# MISSING VALUE IMPUTATION
There are missing values in Gender, Married, Dependents, Self_Employed, LoanAmount, Loan_Amount_Term, and Credit_History features.

We will treat the missing values in all the features one by one.

We can consider these methods to fill the missing values:

For numerical variables: imputation using mean or median
For categorical variables: imputation using mode
There are very few missing values in Gender, Married, Dependents, Credit_History, and Self_Employed features so we can fill them using the mode of the features.
"""

# We need to fill null values with mean and median using missingno package
import missingno as msno
# list of how many percentage values are missing
train

train.isna().sum()
# round((loan_train.isna().sum() / len(loan_train)) * 100, 2)

msno.bar(train)

msno.matrix(train )

# As we can see here, there are too many columns missing with small amount of null values so we use mean amd mode to replace with NaN values.
train['Credit_History'].fillna(train['Credit_History'].mode(), inplace=True) # Mode
test['Credit_History'].fillna(test['Credit_History'].mode(), inplace=True) # Mode


train['LoanAmount'].fillna(train['LoanAmount'].mean(), inplace=True) # Mean
test['LoanAmount'].fillna(test['LoanAmount'].mean(), inplace=True) # Mean

# convert Categorical variable with Numerical values.Loan_Status feature boolean values, 
# So we replace Y values with 1 and N values with 0 and same for other Boolean types of columns
train.Loan_Status = train.Loan_Status.replace({"Y": 1, "N" : 0})
# test.Loan_Status = loan_test.Loan_Status.replace({"Y": 1, "N" : 0}) 

train.Gender = train.Gender.replace({"Male": 1, "Female" : 0})
test.Gender = test.Gender.replace({"Male": 1, "Female" : 0})

train.Married = train.Married.replace({"Yes": 1, "No" : 0})
test.Married = test.Married.replace({"Yes": 1, "No" : 0})

train.Self_Employed = train.Self_Employed.replace({"Yes": 1, "No" : 0})
test.Self_Employed = test.Self_Employed.replace({"Yes": 1, "No" : 0})

train['Gender'].fillna(train['Gender'].mode()[0], inplace=True)
test['Gender'].fillna(test['Gender'].mode()[0], inplace=True)

train['Dependents'].fillna(train['Dependents'].mode()[0], inplace=True)
test['Dependents'].fillna(test['Dependents'].mode()[0], inplace=True)

train['Married'].fillna(train['Married'].mode()[0], inplace=True)
test['Married'].fillna(test['Married'].mode()[0], inplace=True)

train['Credit_History'].fillna(train['Credit_History'].mean(), inplace=True)
test['Credit_History'].fillna(test['Credit_History'].mean(), inplace=True)

# Here, Property_Area, Dependents and Education has multiple values so now we can use LabelEncoder from sklearn package
from sklearn.preprocessing import LabelEncoder
feature_col = ['Property_Area','Education', 'Dependents']
le = LabelEncoder()
for col in feature_col:
    train[col] = le.fit_transform(train[col])
    test[col] = le.fit_transform(test[col])

"""**Finally, We have all the features with numerical values**

**3. Data Visualizations**
"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
sns.set_style('dark')

train
train.plot(figsize=(18, 8))
plt.show()

plt.figure(figsize=(18, 6))
plt.subplot(1, 2, 1)


train['ApplicantIncome'].hist(bins=10)
plt.title("Loan Application Amount ")

plt.subplot(1, 2, 2)
plt.grid()
plt.hist(np.log(train['LoanAmount']))
plt.title("Log Loan Application Amount ")

plt.show()

plt.figure(figsize=(18, 6))
plt.title("Relation Between Applicatoin Income vs Loan Amount ")

plt.grid()
plt.scatter(train['ApplicantIncome'] , train['LoanAmount'], c='k', marker='x')
plt.xlabel("Applicant Income")
plt.ylabel("Loan Amount")
plt.show()

plt.figure(figsize=(12, 6))
plt.plot(train['Loan_Status'], train['LoanAmount'])
plt.title("Loan Application Amount ")
plt.show()

plt.figure(figsize=(12,8))
sns.heatmap(train.corr(), cmap='coolwarm', annot=True, fmt='.1f', linewidths=.1)
plt.show()

"""### In this heatmap, we can clearly seen the relation between two variables

## **Feature Engineering**
Based on the domain knowledge, we can come up with new features that might affect the target variable. We will create the following three new features:

**Total Income** — As discussed during bivariate analysis we will combine the Applicant Income and Co-applicant Income. If the total income is high, the chances of loan approval might also be high.
**EMI** — EMI is the monthly amount to be paid by the applicant to repay the loan. The idea behind making this variable is that people who have high EMI’s might find it difficult to pay back the loan. We **can** calculate the EMI by taking the ratio of the loan amount with respect to the loan amount term.
**Balance Incom**e — This is the income left after the EMI has been paid. The idea behind creating this variable is that if this value is high, the chances are high that a person will repay the loan and hence increasing the chances of loan approval.
"""

train['Total_Income']=train['ApplicantIncome']+train['CoapplicantIncome']
test['Total_Income']=test['ApplicantIncome']+test['CoapplicantIncome']
sns.distplot(train['Total_Income']) 
# Let’s check the distribution of Total Income.

train['EMI']=train['LoanAmount']/train['Loan_Amount_Term']
test['EMI']=test['LoanAmount']/test['Loan_Amount_Term']
sns.distplot(train['EMI'])

# Let’s check the distribution of the EMI variable.

train['Balance Income'] = train['Total_Income']-(train['EMI']*1000)
test['Balance Income']=test['Total_Income']-(test['EMI']*1000)
sns.distplot(train['Balance Income'])
# Let’s check the distribution of the Balance Income

"""### **4. Choose ML Model.**
In this step, We have a lots of Machine Learning Model from sklearn package, and we need to decide which model is give us the better performance. then we use that model in final stage and send to the production level.
## (** Model Building Process** - After creating new features, we can continue the model building process. So we will start with the logistic regression model and then move over to more complex models like RandomForest and XGBoost. We will build the following models in this section.)
"""

# import ml model from sklearn pacakge

from sklearn.linear_model import LogisticRegression

from sklearn.metrics import accuracy_score

"""First of all, we are use LogisticRegression from sklearn.linear_model package. Here is the little information about LogisticRegression.

Logistic Regression is a classification algorithm. It is used to predict a binary outcome (1 / 0, Yes / No, and True / False) given a set of independent variables. To represent binary / categorical outcome, we use dummy variables. You can also think of logistic regression as a special case of linear regression when the outcome variable is categorical, where we are using log of odds as the dependent variable.
"""

# Let's build the model
logistic_model = LogisticRegression()

"""# **5. Traing the ML Model**
Before fitting the model, We need to decide how many feature are available for testing and training, then after complete this step. fitt the model
Currently, we are using Credit_History', 'Education', 'Gender features for training so let's create train and test variables
"""

train_features = ['Credit_History', 'Education', 'Gender']

x_train = train[train_features].values
y_train = train['Loan_Status'].values

x_test = test[train_features].values
logistic_model.fit(x_train, y_train)

"""**6. Predict Model**"""

# Predict the model for testing data
predicted = logistic_model.predict(x_test)
# check the coefficeints of the trained model
print('Coefficient of model :', logistic_model.coef_)
# check the intercept of the model
print('Intercept of model',logistic_model.intercept_)
# Accuray Score on train dataset
# accuracy_train = accuracy_score(x_test, predicted)
score = logistic_model.score(x_train, y_train)
print('accuracy_score overall :', score)
print('accuracy_score percent :', round(score*100,2))

# predict the target on the test dataset
predict_test = logistic_model.predict(x_test)
print('Target on test data',predict_test)

x=len(predict_test)
print(x)


numbers = predict_test

zeros = []
ones = []

for num in numbers:
    if num == 0:
        zeros.append(num)
    else:
        ones.append(num)

zeros_count = len(zeros)
ones_count = len(ones)

print("Zeros:", zeros, "Count:", zeros_count)
print("Ones:", ones, "Count:", ones_count)
